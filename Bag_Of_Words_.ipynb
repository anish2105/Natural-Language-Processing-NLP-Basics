{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsOP7n4EIk3iRBSj7X1sJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish2105/Natural-Language-Processing-NLP-Basics/blob/main/Bag_Of_Words_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vScBruiI7_Z",
        "outputId": "af7b3845-af9d-424f-ab2b-2dcc95dfe0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', 'to', 'great', 'experience', 'learning', ',', 'now', 'start', 'learning']\n",
            "['learning', 'is', 'a', 'good', 'practice']\n",
            "['welcome', 'to', 'great', 'experience', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
            "['welcome', 'great', 'experience', 'learning', 'now', 'start', 'good', 'practice']\n",
            "[1, 1, 1, 2, 1, 1, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "def vectorize(tokens):\n",
        "    ''' This function takes list of words in a sentence as input \n",
        "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
        "    word is not present in tokens and count of token if present.'''\n",
        "    vector=[]\n",
        "    for w in filtered_vocab:\n",
        "        vector.append(tokens.count(w))\n",
        "    return vector\n",
        "def unique(sequence):\n",
        "    '''This functions returns a list in which the order remains \n",
        "    same and no item repeats.Using the set() function does not \n",
        "    preserve the original ordering,so i didnt use that instead'''\n",
        "    seen = set()\n",
        "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
        "\n",
        "stopwords = ['to','is','a']\n",
        "\n",
        "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
        "\n",
        "string1=\"Welcome to great experience Learning , Now start learning\"\n",
        "string2=\"Learning is a good practice\"\n",
        "\n",
        "string1 = string1.lower()\n",
        "string2 = string2.lower()\n",
        "\n",
        "token1 = string1.split()\n",
        "token2 = string2.split()\n",
        "print(token1)\n",
        "print(token2)\n",
        "\n",
        "vocab = unique(token1+token2)\n",
        "print(vocab)\n",
        "\n",
        "filtered_vocab = []\n",
        "\n",
        "for w in vocab:\n",
        "  if w not in stopwords and w not in special_char:\n",
        "    filtered_vocab.append(w)\n",
        "print(filtered_vocab)\n",
        "\n",
        "#convert sentences into vectords\n",
        "vector1 = vectorize(token1)\n",
        "print(vector1)\n",
        "vector2 = vectorize(token2)\n",
        "print(vector2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content” to the model as rarer but perhaps domain specific words.\n",
        "\n",
        "One approach is to rescale the frequency of words by how often they appear in all documents, so that the scores for frequent words like “the” that are also frequent across all documents are penalized.\n",
        "\n",
        "This approach to scoring is called Term Frequency – Inverse Document Frequency, or TF-IDF for short"
      ],
      "metadata": {
        "id": "Gbbq00EnMk0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer , TfidVectorizer\n",
        "\n",
        "sentence_1=\"This is a good job.I will not miss it for anything\"\n",
        "sentence_2=\"This is not good at all\"\n",
        "\n",
        "print('Without Smoothing')\n",
        "\n",
        "tf_idf_vec = TfidVectorizer(use_idf = True,\n",
        "                            smooth_idf = False,\n",
        "                            ngram_range = (1,1) , stop_words = 'english')\n",
        "\n"
      ],
      "metadata": {
        "id": "gzJO6vUKKxVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}